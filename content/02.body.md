## Introduction

### Node degree

![Degree figure](){#fig:degree width="100%"}

### Edge prediction

### Feature-degree correlation


## Methods

### Network permutation

### XSwap algorithm

Hanhijärvi, et al. presented XSwap [@doi:10.1137/1.9781611972795.67], an algorithm for the randomization ("permutation") of unweighted networks (Figure {@fig:algo}A).
The algorithm picks two existing edges at random, and if the edges constitute a valid swap, exchanges the targets between the edges (Figure {@fig:xswap}).

![Graphical representation of the XSwap algorithm applied to two edges.
The algorithm preserves both the source- and target-degree for all nodes.](images/xswap_figure.png){#fig:xswap width="50%"}

To allow greater flexibility, we modified the algorithm by adding two parameters, "`allow_self_loops`", and "`allow_antiparallel`" that allow a greater variety of network types to be permuted (Figure {@fig:algo}B).
Specifically, two chosen edges constitute a valid swap if they preserve degree for all four involved nodes and do not violate the above condition options.
The motivation for these generalizations is to make the permutation method applicable both to directed and undirected graphs, as well as to networks with different types of nodes, variously called multipartite, heterogeneous, or multimodal networks.
We provide documentation for parameter choices depending on the type of network being permuted in the GitHub repository (https://github.com/hetio/xswap).
The original algorithm and our proposed modification are given in Figure {@fig:algo}.

![
  **A.** XSwap algorithm due to Hanhijärvi, et al. [@doi:10.1137/1.9781611972795.67].
  **B.** Proposed modification to XSwap algorithm](images/xswap_algos.png){#fig:algo width="100%"}

### Edge prior

We introduce the "edge prior" to quantify the probability that two nodes are connected based only on their degree.
The edge prior can be estimated using the maximum likelihood estimate for the binomial distribution success probability--the fraction of permuted networks in which a given edge exists.
Based only on permuted networks, the edge prior does not contain any information about the true edges in the (unpermuted) network.
The edge prior is a numerical feature that can be computed for each node pair in a network, and we compared its ability to predict edges in three tasks, discussed below.

### Edge prior approximation

We also considered the possibility that the probability of an edge existing across permuted networks could be written as a closed form equation involving the node pair's degree.
We were unable to find a closed-form solution giving the edge prior without assuming independent node pairs, which we believe is incorrect for XSwap.
Nonetheless, we discovered a good approximation to the edge prior for networks with many nodes and relatively low edge density.

Let $m$ be the total number of edge in the network, and $d(u_i)$, $d(v_j)$ be the source and target degrees of a node pair, respectively.
A good approximation of the edge prior is given by the following:
\begin{equation}
    P_{i,j} = \frac{d(u_i) d(v_j)}{\sqrt{(d(u_i) d(v_j))^2 + (m - d(u_i) - d(v_j) + 1)^2}}
\end{equation}

Further discussion of this approximate edge prior and an derivation are available in [the supplement](#approx-prior-supp).

### Prediction tasks

### Degree-grouping

### Implementation and source code


## Results

### Node degree

We found two examples of node degree bias in the PPI and TF-TG networks we investigated.
Figure {@fig:degree-bias} shows node degree in separate networks for the same type of data.
Depending on the methods by which the represented data were generated, networks of the same type of data may assign very different degree to the same nodes (Figure {@fig:degree-bias}B), and they may have overall degree distributions that differ greatly (Figure {@fig:degree-bias}A).

![**A.** Degree distributions of networks with and without degree bias can be very different.
  Protein-protein interaction (PPI) literature-derived network from STRING [@doi:10.1093/nar/gky1131], high-throughput-derived networks from Rual et al. [@doi:10.1038/nature04209] and Rolland et al. [@doi:10.1016/j.cell.2014.10.050].
  Transcription factor-target gene (TF-TG) literature-derived network from Han et al. [@doi:10.1093/nar/gkx1013] and high-throughput network from Lachmann et al. [@doi:10.1093/bioinformatics/btq466].
  BioRxiv data from Rxivist [@doi:10.1101/515643; @doi:10.5281/zenodo.2566421] had edges split between those existing before and during/after 2018.
  **B.** Uniform random sampling edges produces node degrees that are linearly correlated with those in the unsampled network.
  70% of literature edges were sampled with uniform probability.
  Systematically-derived and literature-derived networks do not produce linearly correlated node degree.
  Assuming that the inclusion of an edge in the systematic network is independent of node degree, the systematic degree should be linearly correlated with the true number of relationships.
  Edges found in literature-derived networks represent a non-random sample from systematically-derived edges.
  Shown are the same PPI and TF-TG networks as in subfigure A.
](https://github.com/greenelab/xswap-analysis/raw/d7181e64a5c90f9720ab453334892aba164651e7/img/degree_bias.png){#fig:degree-bias width="100%"}

Edge prediction methods tend to be heavily influenced by degree (Figure {@fig:feature-degree}).

![Common edge-prediction features are correlated with node degree.
  Shown are five features (Supplemental table {@tbl:edge-prediction}) computed on STRING protein-protein interaction network [@doi:10.1093/nar/gky1131] and the product of the node pair's source and target degrees.
  All five features show a positive relationship with degree, though the magnitude of this correlation is highly variable.
](https://github.com/greenelab/xswap-analysis/raw/d7181e64a5c90f9720ab453334892aba164651e7/img/feature_degree.png){#fig:feature-degree width="100%"}


![Discrimination figure](){#fig:discrimination width="100%"}

![Calibration figure](){#fig:calibration width="100%"}


## Discussion


## Conclusion
