<!-- Candidate titles: -->

<!-- ## Quantifying the effect of node degree on unweighted network inference methods
## Overcoming degree bias in network link prediction using permutation
## Permuted networks isolate degree bias in link prediction -->


## Outline Level I

General introduction to the idea of permuted networks.
* What is really the point, though? If you make good predictions, who cares about null hypothesis, whatever? We are not contending that this will help you make better predictions, simply that those predictions are less likely to be based on the biased nature of the data, and they are more likely to have captured some sort of higher-order biological relationship.
  * Your null hypothesis is always, basically, that no biological information is being extracted in making predictions, no trend of things that lead to true relationships. So if it makes equally good predictions when the connections are meaningless, your method has just picked up on some confounding factors that lead to predictions, and/or your data are biased toward new connections in well-studied (high-degree) nodes.
* Generally poor understanding of a "null hypothesis" for many network methods. Making a prediction itself doesn't really have any statistical rigor, and you'd only need a null hypothesis if you were testing a prediction.
* Basically, treat features computed on a network as test statistics, and compute their distribution under the null hypothesis using permuted networks.
  * So an interesting test would be to do an actual statistical test using test statistics. For example, test whether mean or standard deviation of features are the same between the actual network and permuted network.

Why they should share some features with the original network
Why a counterfactual is necessary -> If incorrect predictions are made using a functional prediction method, then there must be confounding factors that bias the data in some way toward making false predictions. -> We think that major confounding factors are density and degree.

Discussion of why density is worth correcting

Discussion of why node degree is worth correcting
 - Graphs of node degree distribution by high throughput vs literature curated, etc.

Potentially want to account for other data as well, such as neighborhood structure
 - Would need to justify this also
 - Show graphs of metrics quantifying the neighborhood structure for different data sources

Things to address in this section:
* Why you permute networks as opposed to generating them randomly
  * Maybe worth doing a comparison of "performance" with Chung-Lu vs XSwap vs neighborhood-preserving
* Mention weighted networks and why this wouldn't work in those cases. What kind of method could work?

Hypothesis: By comparing the performance of unweighted network inference methods to performance on networks generated by degree-preserving randomization, one can isolate the predictive performance due to node degree, and thus generate a better counterfactual for unsupervised learning approaches.

## Outline Level II

* Introduction
  * Why is this work important?
      * ?I think? that many previous methods have not worried about picking up on biology and and just worried about prediction accuracy. This really should be demonstrated for the paper.
      * It's possible that people doing graph reconstruction disproportionately drop edges through high-degree nodes, so the holdout data to be predicted would disproportionately have high-degree nodes (though could the same be true for negative examples?)
  * What is known about this topic?
      * Should go back through original paper and figure out what they saw as the intended use of this method
      * Other ways to generate a null distribution for network based inference methods?
  * What is my hypothesis?
      * Comparing the performance of a network-based inference method to performance on permuted networks gives an indication either of the extent to which the method picks up on connectivity more than biology, or the extent to which the data to-be-predicted contains the kinds of examples that you want to be able to predict in the future. (That is, if your examples are all edges between two really high degree nodes, you shouldn't expect to be able to predict small-degree node connections.)
  * What are my objectives?
      * Demonstrate a general framework for quantifying the density, degree, and neighborhood effects in unweighted network inference methods.

* Methods
  * Description of XSwap, the neighborhood-preserving randomization, Chung-Lu, and our implementations
  * Graphs showing percent swapped, speed, etc.
  * Bayesian framework for thinking about pairwise features, feature priors, etc. Mathematical writeup.

* Results
  * How can we quantify the success of our method?
    * Can check impact on predictions -> Some sort of rank test
    * There should be some dataset that is probably better than others. Using this method, the difference between performance on that data and other data should increase when using the XSwap approach. This could be the curated vs high-throughput data, or something similar.

* Discussion and Conclusions
  * What are the study's major findings?
      * Hopefully, that we see improved performance on ?the worse data? or the better data? Which should improve more using the permutation approach? I suppose worse data performance should decrease, better data performance should ?increase?
        * So performance will decrease, but you're actually more likely picking up on biology. Can we prove that?
  * What is the significance/implication of the results?
      * Hopefully, the significance is that when you're working with biased networks (which you always are in biology, to some degree), you should examine the predictive nature of those confounders, which can speak either/both to your method or/and your data.

## Expansion of outline into text
<!-- Not worrying about quality of writing, just trying to scaffold the paper for future copy-editing and revision -->
## Introduction

* Introduction
  * Why is this work important?
    * ?I think? that many previous methods have not worried about picking up on biology and and just worried about prediction accuracy. This really should be demonstrated for the paper. Also, no need to critique previous work. Can just say this is a way to quantify its effects. Predictions are cool regardless of whether they pick up on biology or not. May still be clinically-relevant.
    * It's possible that people doing graph reconstruction disproportionately drop edges through high-degree nodes, so the holdout data to be predicted would disproportionately have high-degree nodes (though could the same be true for negative examples?)

<!-- Generic background on network inference methods in systems biology -->
Network edge (or "link") prediction is a classic problem in statistics and machine learning.
While many methods have found success in predicting new links in biological networks (**CITATIONS**), the the predictions made are hugely influenced by aspects of networks that may interfere with the prediction of the desired link-types.
That is, while many link prediction methods are able to recapitulate new or held-out links, insufficient effort has been devoted to an analysis of the types of links being predicted.
For example, consider the task of predicting new interactions in a protein-protein interaction network.
To predict an interaction between ubiquitin and a protein with which it does not yet have a demonstrated connection would be methodologically trivial and the result uninteresting, even though the proteins may have an actual biological interaction.
In some link prediction methods, the true goal is to predict connections that are between relatively low-degree nodes, which in some sense represent more biologically novel interactions.
In the previous example, node degree was an impediment to making biologically *interesting* predictions, despite the fact that undoubtedly many of the predictions made were true interactions.

This discussion is meant to illustrate the following point: biological applications should concern themselves to a greater degree with the prediction of connections that are *novel*, and thus *harder* to predict and more subject to being drowned out in the sea of uninteresting predictions that could be made simply by considering node degree, etc.
Network characteristics like node degree are not confounders toward making true predictions, but they bias prediction away from the prediction of unexpected novel links.
Moreover, while such characteristics may be informative for making predictions, they can vary widely between different networks, and thus also make predictions less replicable on new data and methods less transferrable to novel data.

Several network characteristics significantly affect many network-based prediction methods, especially those involving edge prediction.
While the full scope of potentially-influential network characteristics is endless, we believe that among the most important are network density, node degree, and neighborhood structure.
These characteristics are to varying degrees influenced by faithfully representing biology and by bias in the knowledge gained through scientific study and knowledge that has been incorporated into conceptual networks.
Some data and some inference tasks may or may not make the incorporation of these characteristics scientifically meaningful.
For many tasks, though, these network characteristics arise through biased means.
This can be demonstrated by comparing the distributions of node degree between networks generated from literature-curated relationships versus networks whose relationships were generated through relatively unbiased high-throughput screening methods (**FIGURE LINK HERE** maybe also density and neighborhood).

The biases and limitations in many biomedical data resources and the propensity for network inference methods to pick up on these confounding factors rather than biologically-relevant patterns of connection indicate the need for a general framework for quantifying and a method-specific framework for accounting-for these biases in network inference methods.
<!-- Previous work by others -->
* What is known about this topic?
    * Should go back through original paper and figure out what they saw as the intended use of this method
    * Other ways to generate a null distribution for network based inference methods?

In this work we present a network permutation method for the isolation of density/degree/neighborhood effects from the data/inference method.
Each use-case of our method requires an understanding of the network characteristics that are relevant and those which are confounding.
For example, in some applications or data resources, the neighborhood structure of a network is of central importance and does not represent a counfounding effect.
In other applications, such as, for example, the network-based prediction of protein-protein interactions, the number of interactions that a single protein makes could be due either/both to the protein having many true interactions or/and due to the protein being more well studied than the other proteins in the network.
Our methods are able to account for three confounders, density, degree, and neighborhood structure.
While we provide a detailed framework for edge prediction methods, our method is applicable to any network inference approaches involving networks with unweighted edges.

A procedure for degree-preserving randomizations that is applicable to bipartite graphs.

## Methods

* Methods
  * Description of XSwap, Chung-Lu, Erdos-Renyi, and our implementations
  * Graphs showing percent swapped, speed, etc.
  * Bayesian framework for thinking about pairwise features, feature priors, etc. Mathematical writeup.


<!-- Our method's fundamental network randomization procedure involves degree-preserving edge swaps. -->
We propose a method of degree-preserving edge swaps to generate random networks.
While this procedure is not guaranteed to sample in an unbiased fashion from the space of all networks with a given degree sequence, the procedure is simpler and faster than alternatives which guarantee a degree sequence.
Methods which generate networks having an *expected* degree sequence equal to a given degree sequence are also viable, and we include a comparison of such methods in the results section.
Briefly, we believe that both methods are potentially viable, though we prefer guaranteed degrees with potentially biased sampling to unbiased samples with potentially different degree sequence.

Hanhijärvi, et al. presented XSwap (**CITATION NEEDED**), an algorithm for the randomization ("permutation") of unweighted networks (**FIGURE ALGORITHM BELOW**).
The algorithm picks two existing edges at random, and if the edges constitute a valid swap, exchanges the targets between the edges.
To allow greater flexibility for heterogeneous biomedical networks with various types of relationships, we added two option parameters, "allow_self_loops", and "allow_antiparallel" that allow a greater range of possibly valid swaps.
Specifically, two chosen edges constitute a valid swap if they preserve degree for all four involved nodes and do not violate the above condition options.
The original algorithm and our proposed modification are shown below.

<pre id="xswap" style="display:none">
    \begin{algorithm}
    \caption{XSwap algorithm due to Hanhijärvi, et al.}
    \begin{algorithmic}

    \STATE \textbf{Input:} Undirected graph $G$, distribution $\rho$, and number of steps $T$
    \STATE \textbf{Output:} Edge-swapped graph $G_s$

    \FOR{i = 1,...,T}
        \STATE Select two edges $(i, j)$, $(k, l) \in E(G_s)$
        \IF{$(i, l)\not\in E(G_s)$ and $(k, j)\not\in E(G_s)$}
            \STATE $E(\widehat{G_s}) \gets E(G_s) \setminus \{(i, j), (k, l)\} \cup \{(i, l), (k, j)\}$
            \STATE $G_s \gets \widehat{G_s}$ with probability $\mathrm{min}(\rho(\widehat{G_s}) / \rho(G_s), 1)$
        \ENDIF
    \ENDFOR

    \end{algorithmic}
    \end{algorithm}
</pre>
<script type="text/javascript">
    var xswap = document.getElementById("xswap").textContent;
    pseudocode.render(xswap, document.body, {
        lineNumber: false,
        indentSize: '1.2em'
    });
</script>


<pre id="our-algo" style="display:none">
    \begin{algorithm}
    \caption{Proposed modified algorithm for multi-type network permutation}
    \begin{algorithmic}
    \STATE \textbf{Input:}
    Undirected graph $G$,
    number of steps $T$,
    booleans \texttt{allow\_antiparallel} and \texttt{allow\_loops}
    \STATE \textbf{Output:} Edge-swapped graph $G_T$
    \STATE \textbf{Initialize} $G_0 \gets G$
    \FOR{i = 1,...,T}
        \STATE Select two edges $(i, j)$, $(k, l) \in E(G_{i-1})$
        <!-- \STATE $E(\widehat{G_i}) \gets (E(G_i) \setminus \{(i, j), (k, l)\}) \cup \{(i, l), (k, j)\}$ -->
        \STATE \textbf{condition 1} $(i, l)\not\in E(G_{i-1})$ \AND $(k, j)\not\in E(G_{i-1})$
        \STATE \textbf{condition 2} \texttt{allow\_antiparallel} or ($(l, i) \not\in E(G_{i-1})$ and $(k, j) \not \in E(G_{i-1})$)
        \STATE \textbf{condition 3} \texttt{allow\_loops} or ($i \neq l$ and $k \neq j$))
        \IF{all conditions met}
            \STATE $E(G_{i}) \gets (E(G_{i-1}) \setminus \{(i, j), (k, l)\}) \cup \{(i, l), (k, j)\} $
        \ELSE
            \STATE $G_{i} \gets G_{i-1}$
        \ENDIF
    \ENDFOR
    \end{algorithmic}
    \end{algorithm}
</pre>
<script type="text/javascript">
    var ours = document.getElementById("our-algo").textContent;
    pseudocode.render(ours, document.body, {
        lineNumber: false,
        indentSize: '1.2em'
    });
</script>

Our method applies degree-preserving network randomization (DPR) to network inference on a node or node-pair basis.
Previous work has demonstrated the applicability of DPR to the generation of random networks that are more similar to real networks than those which could be generated using previous random-graph generation methods.
For example, Dekker used DPR to generate social networks that were similar to real networks.
**Other examples**
Unlike previous works, our method seeks to use DPR to inform inference at a node or node-pair scale, rather than at the scale of whole-network summary features.
Consequently, our approach requires a node or node-pair feature generation method such as random walk with restart (RWR) that can be treated like the test statistic in a permutation test.
By comparing features computed on an actual network to the distribution of those features computed on DPR-generated graphs, one can isolate the effect of node degree on the features of interest.
Computing a feature on DPR-generated networks allows one to estimate the distribution of that feature under the null hypothesis that the network's edges are not informative.
Moreover, the degree-preserving aspect of DPR relative to other methods for the generation of random networks allows the null hypothesis to be more compact.
The null hypothesis must only state that the network's specific connections are not informative.
This specificity is more helpful than would be possible under a testing paradigm in which one needed also to make assumptions about degree and network characteristics.
For example, the null hypothesis that the network's edges are distributed randomly is far easier to reject, as most real networks will follow a non-random degree distribution.

Our method is applicable to bipartite graphs.
This is a major advantage of our method over others because the generation of a random bipartite graph with specified vertex degree sequences for both color classes is possible with a Havel-Hakimi type algorithm, but such graphs have a

## Results

* Results
  * How can we quantify the success of our method?
    * Can check impact on predictions -> Some sort of rank test
    * There should be some dataset that is probably better than others. Using this method, the difference between performance on that data and other data should increase when using the XSwap approach. This could be the curated vs high-throughput data, or something similar.

* Discussion and Conclusions
  * What are the study's major findings?
      * Hopefully, that we see improved performance on ?the worse data? or the better data? Which should improve more using the permutation approach? I suppose worse data performance should decrease, better data performance should ?increase?
        * So performance will decrease, but you're actually more likely picking up on biology. Can we prove that?
  * What is the significance/implication of the results?
      * Hopefully, the significance is that when you're working with biased networks (which you always are in biology, to some degree), you should examine the predictive nature of those confounders, which can speak either/both to your method or/and your data.

